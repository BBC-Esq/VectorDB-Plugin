<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embedding Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #161b22;
            color: #d0d0d0;
        }
		
		header {
			text-align: center;
			background-color: #3498db;
			color: #333;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}
		
		header a {
			color: #fff;
			text-decoration: none;
		}

		header a:hover {
			text-decoration: underline;
		}
		
		main {
			max-width: 2160px;
			margin: 0 auto;
			padding: 20px;
		}
		
		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}
		
		p {
			text-indent: 25px;
		}

		
		table {
            color: black;
			border-collapse: collapse;
            margin: 25px auto;
        }
		
		thead th {
			background-color: #f69784;
		}

        table, th, td {
            border: 1px solid black;
        }

        th, td {
            padding: 8px 12px;
        }

        .tiny {
            background-color: #e6f7ff;
        }

        .base {
            background-color: #b3e0ff;
        }

        .small {
            background-color: #66c2ff;
        }

        .medium {
            background-color: #3399ff;
        }

        .large {
            background-color: #0073e6;
        }
		
		code {
			background-color: #d0d0d0;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
		
		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		.table-from-second-file, .table-from-second-file th, .table-from-second-file td {
			border-collapse: collapse;
			margin: 25px auto;
			text-align: center;
			padding: 8px;
			border-bottom: 1px solid #ddd;
			color: #000;
		}

		.table-from-second-file th {
			background-color: #f2f2f2;
			color: #000;
		}
		
		.table-from-second-file td {
			color: #fff; /* White color for non-header cells */
			border-bottom: 1px solid #fff;
			border-left: 1px solid #fff;
			border-right: 1px solid #fff;
			text-align: center;
		}
		
		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}
		
		a {
		  color: #0d4885; /* Change this to your desired color */
		}
		a:visited {
		  color: #0d4885; /* Color for visited links */
		}
    </style>
</head>

<body>

	<header>
		<h1>Embedding Models</h1>
	</header>
	
	<main>
	
	<h2>Overview</h2>
	
	<p> This program extracts text from a variety of file formats, splits the text, and then converts the chunks
	vectors in order to be put into a vector database to be searable.  This is commonly referred to as "RAG" or
	"Retrieval Augmented Generaton."  Your query is then used to pull relevant chunks from the vector database
	and your query along with the contexts are sent to the LLM for an answer.</p>

	<h2>Choosing the Correct Model</h2>
	
	<p>Early versions of this program used a wide variety of vector models.  However, most of the models are now
	powerful enough to be good at a variety of tasks.  Therefore, this program now uses two kinds of models.</p>
	
	<p>First, models beginning with <code>sentence-t5</code> are specifically trained to sentences that are as similar
	the one in your query.  They should not be used for general information retrieval or question answering.</p>
	
	<p>For example, if your query is...</p>
	
	<p><b>Quote for me all sentences that discuss the main character in this book eating food.</b></p>
	
	<p>or...</p>
	
	<p><b>Provide me all sentences verbatim of a court discussing the elements of a defamation claim.</b></p>
	
	<p>...you would receive multiple sentences that mimic your query.</p>
	
	<p>All other models fall within the second category, which can be characterized as "generalist" models.  They excel
	at returning contexts that answer your question and should only be used for that purpose.</p>
	
	<h2 style="color: #f0f0f0;" align="center">Vector Model Characteristics</h2>
	
	<p><code>Max sequence</code></p>
	<p>Refers to the maximum number of tokens (not characters) that a given model can  process at a time.
	This is different than the <code>Chunk Size</code> setting within the Settings tab, which is in characters.</p>
	
	<p><code>"Dimensions"</code></p>
	<p>Refers to how complex the number is that the vector model creates to represent the meaning of a chunk of text.
	A higher number means more nuance, which improves search results, while a lower number means less processing time.</p>
	
	<p><code>Size</code></p>
	<p>Refers to the size on disk.</p>

	<h2 style="color: #f0f0f0;" align="center">Tips</h2>
	
	<p>Always use the highest quality vector model that your system specs and time allow. Most of the models will fit
	comfortably within your amount of VRAM because I have specifically overridden the default <code>batch_size</code> parameter
	within the <code>sentence-transformers</code> library of 32.  It is now optimized for each particular vector model based on my
	extensive testing.  As seen below, using a batch_size of 32 actually decreases performance and unnecessarily increases VRAM usage.</p>
	
	<img src="chart_vector_models.png" alt="Vector Model VRAM Trends"><br>
	<img src="chart_vector_batch.png" alt="Vector Model Compute Time Trends">
	
	<p>If you must modify them, they can be altered within the <code>database_interactions.py</code> script.</p>

</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
