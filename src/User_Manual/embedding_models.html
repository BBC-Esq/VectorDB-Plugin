<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embedding Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #161b22;
            color: #d0d0d0;
        }
		
		header {
			text-align: center;
			background-color: #3498db;
			color: #333;
			padding: 20px;
			position: sticky;
			top: 0;
			z-index: 999;
		}
		
		header a {
			color: #fff;
			text-decoration: none;
		}

		header a:hover {
			text-decoration: underline;
		}
		
		main {
			max-width: 2160px;
			margin: 0 auto;
			padding: 20px;
		}
		
		h1 {
		  color: #333;
		}

		h2 {
		  color: #f0f0f0;
		  text-align: center;
		}
		
		p {
			text-indent: 25px;
		}

		
		table {
            color: black;
			border-collapse: collapse;
            margin: 25px auto;
        }
		
		thead th {
			background-color: #f69784;
		}

        table, th, td {
            border: 1px solid black;
        }

        th, td {
            padding: 8px 12px;
        }

        .tiny {
            background-color: #e6f7ff;
        }

        .base {
            background-color: #b3e0ff;
        }

        .small {
            background-color: #66c2ff;
        }

        .medium {
            background-color: #3399ff;
        }

        .large {
            background-color: #0073e6;
        }
		
		code {
			background-color: #d0d0d0;
			border-radius: 3px;
			padding: 2px 3px;
			font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
			color: #333;
		}
		
		footer {
			text-align: center;
			background-color: #333;
			color: #fff;
			padding: 10px;
		}
		
		.table-from-second-file, .table-from-second-file th, .table-from-second-file td {
			border-collapse: collapse;
			margin: 25px auto;
			text-align: center;
			padding: 8px;
			border-bottom: 1px solid #ddd;
			color: #000;
		}

		.table-from-second-file th {
			background-color: #f2f2f2;
			color: #000;
		}
		
		.table-from-second-file td {
			color: #fff; /* White color for non-header cells */
			border-bottom: 1px solid #fff;
			border-left: 1px solid #fff;
			border-right: 1px solid #fff;
			text-align: center;
		}
		
		img {
			display: block;
			margin: 0 auto;
			max-width: 100%;
			height: auto;
		}
		
		a {
		  color: #0d4885; /* Change this to your desired color */
		}
		a:visited {
		  color: #0d4885; /* Color for visited links */
		}
    </style>
</head>

<body>

	<header>
		<h1>Embedding Models</h1>
	</header>
	
	<main>
	
	<h2>Overview</h2>
	
	<p> This program extracts text from a variety of file formats, splits the text, and then converts the chunks
	vectors in order to be put into a vector database to be searable.  This is commonly referred to as "RAG" or
	"Retrieval Augmented Generaton."  Your query is then used to pull relevant chunks from the vector database
	and your query along with the contexts are sent to the LLM for an answer.</p>

	<h2>Choosing the Correct Model</h2>
	
	<p>Prior version of this program used a variety of different vector models, each with their own unique
	characteristics.  However, most of the models nowadays are generalist and perform quite well on a variety
	of text.  Generally, the size of a model determines its quality but you should experiment with with different
	ones to your liking.</p>
	
	<p>With that being said, models beginning with <code>sentence-t5</code> are trained for a specific task, which
	is to return sentences from the database similar to the one in your query.  These should only be used for that
	specific task.</p>
	
	<p>For example, if your query is...</p>
	
	<p><b>Quote for me all sentences that discuss the main character in this book eating food.</b></p>
	
	<p>or...</p>
	
	<p><b>Provide me all sentences verbatim of a court discussing the elements of a defamation claim.</b></p>
	
	<p>...you would receive multiple sentences that mimic your query.</p>
	
	<p>Overall, almost all of the generalist models selected for this program are good and you should only use the
	<code>sentence-t5</code> models if you have a strong need for that specific task.</p>
	
	<h2 style="color: #f0f0f0;" align="center">Vector Model Characteristics</h2>
	
	<p>The Model card contains characteristics for all vector models as well as a hyperlink to the model's card.</p>
	
	<p><code>Max sequence</code> refers to the maximum number of tokens (not characters) that a given model can
	process at a time.  This is different than the <code>Chunk Size</code> setting within the Settings tab, which
	which refers to the number of characters you want the text split into.</p>
	
	<p>A general rule of thumb is that a "token" is approximately four "characters."  Thus, a vector model with a
	"max sequence" of 512 would be able to process approximately 2048 "characters" at a time.  This is the
	equivalent of setting the <code>Chunk Size</code> setting to 2048 in the Settings tab.</p>
	
	<p>A chunksize of 500-700 is still sufficient for a vast majority of use-cases.  Newer vector models are
	coming out everyday, however, that can process thousands of tokens at a time and those will probably be
	incorporated into this program at some point.  For the time being, simply make sure that the chunk size you
	select falls under the "max sequence" for a given model, otherwise, the model will truncate the text.
	It won't notify you this happens but the search results will degrade.</p>
	
	<p><code>"Dimensions"</code> refers to how nuanced a meaning that the model can discern from text.
	A higher number means that it can discerng more meaning, thus improving search results, and a lower
	number means less processing requirements.</p>
	
	<p><code>Size</code> simply refers to the size on disk if that's a concern for you.</p>
	
	<h2 style="color: #f0f0f0;" align="center">Tips</h2>
	
	<p>Always use "cuda" when creating the database if your hardware allows.</p>
	
	<p>Different chunk sizes produce better results based on the type of text being vectorized.  Experiment
	with different chunks sizes.  The same holds true for the "chunk overlap" setting.</p>
	
	<p>If time is of little concern, always use the highest quality vector model that your system specs allow.
	Most of the models will fit comfortably within your amount of VRAM.  I have specifically overridden the
	"batch size" parameter within the <code>sentence-transformers</code> library to ensure this.  I believe
	that batch size is something like "32," which, as you can see from the below graphs will result in an
	unreasonable increase in VRAM usage and, at a certain point, will ACTUALLY INCREASE the time processing
	time compared to a lower batch size.</p>
	
	<img src="chart_vector_models.png" alt="Vector Model VRAM Trends"><br>
	<img src="chart_vector_batch.png" alt="Vector Model Compute Time Trends">
	
	<p>I've manually set the batch sizes depending on which vector model is being used to make them work efficiently
	on most GPUs.  However, if you are hardcore feel free to modify them within the <code>database_interactions.py</code> script.</p>
	
	<p>Regarding the <code>instructor-xl</code> model, it's batch size is 1 and therefore it should still fit on
	most GPUs comfortably.  In the future, this program will be able to run the models using <code>float</code>
	and/or <code>bfloat</code> precison for approximately twice the performance with little to no quality loss.</p>


</main>

    <footer>
        www.chintellalaw.com
    </footer>
</body>
</html>
